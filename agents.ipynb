{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e993fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ticTacToe import Model, View, Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba4e54",
   "metadata": {},
   "source": [
    "## Tic Tac Toe - Learning to Play Games\n",
    "\n",
    "### Chapters:\n",
    "- Building a sandbox environment\n",
    "- Random Baseline\n",
    "- Minimax (Classical AI)\n",
    "- Q-Learning (Reinforcement Learning & Deep Learning Revolution)\n",
    "- Reinforcement Learning from Human Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda7cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAI:\n",
    "    def __init__(self, model, playerType):\n",
    "        self.model = model\n",
    "        self.playerType = playerType\n",
    "    \n",
    "    def getAvailableMoves(self, board):\n",
    "        \"\"\"Find all empty cells on the board\"\"\"\n",
    "        moves = []\n",
    "        for rowIdx, row in enumerate(board):\n",
    "            for colIdx, cell in enumerate(row):\n",
    "                if cell == \"+\":\n",
    "                    moves.append((rowIdx, colIdx))\n",
    "        return moves\n",
    "    \n",
    "    \n",
    "    def getMove(self, model):\n",
    "        self.model = model\n",
    "        board = model.board\n",
    "        availableMoves = self.getAvailableMoves(board)\n",
    "        return availableMoves[random.randint(0, len(availableMoves) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedAI:\n",
    "    def __init__(self, model, playerType):\n",
    "        self.model = model\n",
    "        self.playerType = playerType\n",
    "        # Set opponent type (if we're X, opponent is O and vice versa)\n",
    "        self.opponentType = \"O\" if playerType == \"X\" else \"X\"\n",
    "    \n",
    "    def getAvailableMoves(self, board):\n",
    "        \"\"\"Find all empty cells on the board\"\"\"\n",
    "        moves = []\n",
    "        for rowIdx, row in enumerate(board):\n",
    "            for colIdx, cell in enumerate(row):\n",
    "                if cell == \"+\":\n",
    "                    moves.append((rowIdx, colIdx))\n",
    "        return moves\n",
    "    \n",
    "    #make check winning move into an exercise!\n",
    "    def checkWinningMove(self, board, player):\n",
    "        \"\"\"\n",
    "        Check if 'player' can win with one move.\n",
    "        Returns the winning position (x, y) if found, otherwise None.\n",
    "        \"\"\"\n",
    "        size = len(board)\n",
    "        \n",
    "        # Check each row for a winning opportunity\n",
    "        for row in range(size):\n",
    "            cells = [board[row][col] for col in range(size)]\n",
    "            if cells.count(player) == size - 1 and cells.count(\"+\") == 1:\n",
    "                col = cells.index(\"+\")\n",
    "                return (row, col)\n",
    "        \n",
    "        # Check each column for a winning opportunity\n",
    "        for col in range(size):\n",
    "            cells = [board[row][col] for row in range(size)]\n",
    "            if cells.count(player) == size - 1 and cells.count(\"+\") == 1:\n",
    "                row = cells.index(\"+\")\n",
    "                return (row, col)\n",
    "        \n",
    "        # Check main diagonal (top-left to bottom-right)\n",
    "        cells = [board[i][i] for i in range(size)]\n",
    "        if cells.count(player) == size - 1 and cells.count(\"+\") == 1:\n",
    "            idx = cells.index(\"+\")\n",
    "            return (idx, idx)\n",
    "        \n",
    "        # Check anti-diagonal (top-right to bottom-left)\n",
    "        cells = [board[i][size - 1 - i] for i in range(size)]\n",
    "        if cells.count(player) == size - 1 and cells.count(\"+\") == 1:\n",
    "            idx = cells.index(\"+\")\n",
    "            return (idx, size - 1 - idx)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def getMove(self, model):\n",
    "        self.model = model\n",
    "        board = model.board\n",
    "        \n",
    "        # Rule 1: If we can win, take the winning move!\n",
    "        winMove = self.checkWinningMove(board, self.playerType)\n",
    "        if winMove:\n",
    "            return winMove\n",
    "        \n",
    "        # Rule 2: If opponent can win, block them!\n",
    "        blockMove = self.checkWinningMove(board, self.opponentType)\n",
    "        if blockMove:\n",
    "            return blockMove\n",
    "        \n",
    "        # Rule 3: Otherwise, pick a random available move\n",
    "        availableMoves = self.getAvailableMoves(board)\n",
    "        return availableMoves[random.randint(0, len(availableMoves) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "py18jyx0wlq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Q-LEARNING AI - The Simplest Reinforcement Learning\n",
    "# =============================================================================\n",
    "#\n",
    "# THE BIG IDEA:\n",
    "# Instead of programming rules (like RuleBasedAI) or searching all possibilities\n",
    "# (like Minimax), we let the AI LEARN from experience by playing many games.\n",
    "#\n",
    "# KEY CONCEPT - THE Q-TABLE:\n",
    "# A dictionary that maps: (board_state, action) → \"how good is this move?\"\n",
    "#\n",
    "#   Q-Table (simplified example):\n",
    "#   ┌─────────────────────┬────────┬─────────┐\n",
    "#   │ State (board)       │ Action │ Q-Value │\n",
    "#   ├─────────────────────┼────────┼─────────┤\n",
    "#   │ \"X++|+++|+++\"       │ (1,1)  │  0.8    │  ← \"center is good when X plays first\"\n",
    "#   │ \"X++|+++|+++\"       │ (0,1)  │  0.2    │  ← \"edge is okay\"\n",
    "#   │ \"XO+|+++|+++\"       │ (1,1)  │  0.9    │  ← \"center is great here too\"\n",
    "#   └─────────────────────┴────────┴─────────┘\n",
    "#\n",
    "# HOW IT LEARNS (after each game):\n",
    "# 1. If we WON:  increase Q-values for all moves we made (reward = +1)\n",
    "# 2. If we LOST: decrease Q-values for all moves we made (reward = -1)  \n",
    "# 3. If TIE:     slightly increase Q-values (reward = +0.5)\n",
    "#\n",
    "# EXPLORATION VS EXPLOITATION:\n",
    "# - Exploration: Try random moves to discover new strategies (epsilon chance)\n",
    "# - Exploitation: Use the best known move from Q-table (1 - epsilon chance)\n",
    "# - Start with high exploration, gradually reduce it as we learn\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "class QLearningAI:\n",
    "    def __init__(self, model, playerType, epsilon=0.3, learning_rate=0.5):\n",
    "        self.model = model\n",
    "        self.playerType = playerType\n",
    "        \n",
    "        # THE Q-TABLE: stores (state, action) → value\n",
    "        # Key: (board_as_string, (row, col))\n",
    "        # Value: how good we think this move is (starts at 0)\n",
    "        self.q_table = {}\n",
    "        \n",
    "        # EXPLORATION RATE: probability of trying a random move\n",
    "        # Higher = more exploration, Lower = more exploitation\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # LEARNING RATE: how much we update Q-values after each game\n",
    "        # Higher = learn faster but less stable\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Track moves made during current game (for updating Q-values later)\n",
    "        self.move_history = []\n",
    "    \n",
    "    def boardToString(self, board):\n",
    "        \"\"\"\n",
    "        Convert board to a string so we can use it as a dictionary key.\n",
    "        Example: [[\"X\",\"+\",\"+\"],[\"O\",\"+\",\"+\"],[\"+\",\"+\",\"+\"]] → \"X++|O++|+++\"\n",
    "        \"\"\"\n",
    "        return \"|\".join([\"\".join(row) for row in board])\n",
    "    \n",
    "    def getAvailableMoves(self, board):\n",
    "        \"\"\"Find all empty cells\"\"\"\n",
    "        moves = []\n",
    "        for rowIdx, row in enumerate(board):\n",
    "            for colIdx, cell in enumerate(row):\n",
    "                if cell == \"+\":\n",
    "                    moves.append((rowIdx, colIdx))\n",
    "        return moves\n",
    "    \n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "        Get the Q-value for a (state, action) pair.\n",
    "        Returns 0 if we've never seen this combination before.\n",
    "        \"\"\"\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "    \n",
    "    def getMove(self, model):\n",
    "        \"\"\"\n",
    "        Choose a move using epsilon-greedy strategy:\n",
    "        - With probability epsilon: pick a RANDOM move (explore)\n",
    "        - With probability 1-epsilon: pick the BEST known move (exploit)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        board = model.board\n",
    "        state = self.boardToString(board)\n",
    "        availableMoves = self.getAvailableMoves(board)\n",
    "        \n",
    "        # EXPLORATION: pick a random move\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(availableMoves)\n",
    "        \n",
    "        # EXPLOITATION: pick the move with highest Q-value\n",
    "        else:\n",
    "            # Find the best move (highest Q-value)\n",
    "            best_value = float('-inf')\n",
    "            best_moves = []\n",
    "            \n",
    "            for move in availableMoves:\n",
    "                q_value = self.getQValue(state, move)\n",
    "                if q_value > best_value:\n",
    "                    best_value = q_value\n",
    "                    best_moves = [move]\n",
    "                elif q_value == best_value:\n",
    "                    best_moves.append(move)\n",
    "            \n",
    "            # If multiple moves have same Q-value, pick randomly among them\n",
    "            action = random.choice(best_moves)\n",
    "        \n",
    "        # Remember this move so we can update Q-values after the game\n",
    "        self.move_history.append((state, action))\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    #make this into an excerise!\n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Update Q-values after a game ends.\n",
    "        Called with reward: +1 (win), -1 (loss), or +0.5 (tie)\n",
    "        \n",
    "        Simple update rule:\n",
    "        Q(state, action) = Q(state, action) + learning_rate * (reward - Q(state, action))\n",
    "        \n",
    "        This moves the Q-value toward the reward we received.\n",
    "        \"\"\"\n",
    "        # Update Q-value for each move we made this game\n",
    "        for (state, action) in self.move_history:\n",
    "            old_value = self.getQValue(state, action)\n",
    "            # Move Q-value toward the reward\n",
    "            new_value = old_value + self.learning_rate * (reward - old_value)\n",
    "            self.q_table[(state, action)] = new_value\n",
    "        \n",
    "        # Clear history for next game\n",
    "        self.move_history = []\n",
    "    \n",
    "    def decayEpsilon(self, decay_rate=0.99):\n",
    "        \"\"\"\n",
    "        Reduce exploration over time.\n",
    "        As we learn more, we should exploit our knowledge more.\n",
    "        \"\"\"\n",
    "        self.epsilon = self.epsilon * decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ucrrlmpo3sc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING THE Q-LEARNING AI\n",
    "# =============================================================================\n",
    "# We train by playing many games against a RandomAI opponent.\n",
    "# After each game, we update Q-values based on win/loss/tie.\n",
    "# =============================================================================\n",
    "\n",
    "def trainQLearning(num_games=5000):\n",
    "    \"\"\"Train Q-Learning AI by playing against RandomAI\"\"\"\n",
    "    \n",
    "    # Create the Q-Learning agent (starts knowing nothing)\n",
    "    q_ai = QLearningAI(model=None, playerType=\"X\", epsilon=0.5, learning_rate=0.5)\n",
    "    \n",
    "    # Track results for plotting\n",
    "    results = {\"wins\": 0, \"losses\": 0, \"ties\": 0}\n",
    "    win_rates = []  # Track win rate over time\n",
    "    \n",
    "    for game_num in range(num_games):\n",
    "        # Create fresh game\n",
    "        board = [[\"+\" for _ in range(3)] for _ in range(3)]\n",
    "        playerX = Player(\"X\")\n",
    "        playerO = Player(\"O\")\n",
    "        model = Model(board, \"X\", playerX, playerO)\n",
    "        \n",
    "        # Create opponent (random player)\n",
    "        opponent = RandomAI(model=None, playerType=\"O\")\n",
    "        \n",
    "        # Play the game\n",
    "        while not model.gameOver:\n",
    "            if model.turn == \"X\":\n",
    "                # Q-Learning AI's turn\n",
    "                x, y = q_ai.getMove(model)\n",
    "            else:\n",
    "                # Opponent's turn\n",
    "                x, y = opponent.getMove(model)\n",
    "            \n",
    "            model.makeMove(x, y)\n",
    "            model.checkGameOver()\n",
    "            \n",
    "            if not model.gameOver:\n",
    "                model.turn = \"O\" if model.turn == \"X\" else \"X\"\n",
    "        \n",
    "        # Game over - give reward to Q-Learning AI\n",
    "        if model.winner == \"X\":\n",
    "            q_ai.learn(reward=1.0)   # Win!\n",
    "            results[\"wins\"] += 1\n",
    "        elif model.winner == \"O\":\n",
    "            q_ai.learn(reward=-1.0)  # Loss\n",
    "            results[\"losses\"] += 1\n",
    "        else:\n",
    "            q_ai.learn(reward=0.5)   # Tie\n",
    "            results[\"ties\"] += 1\n",
    "        \n",
    "        # Reduce exploration over time\n",
    "        q_ai.decayEpsilon(decay_rate=0.999)\n",
    "        \n",
    "        # Track win rate every 100 games\n",
    "        if (game_num + 1) % 100 == 0:\n",
    "            recent_wr = results[\"wins\"] / (game_num + 1) * 100\n",
    "            win_rates.append(recent_wr)\n",
    "    \n",
    "    # Print final results\n",
    "    total = num_games\n",
    "    print(f\"Training complete! Results after {num_games} games:\")\n",
    "    print(f\"  Wins:   {results['wins']} ({results['wins']/total*100:.1f}%)\")\n",
    "    print(f\"  Losses: {results['losses']} ({results['losses']/total*100:.1f}%)\")\n",
    "    print(f\"  Ties:   {results['ties']} ({results['ties']/total*100:.1f}%)\")\n",
    "    print(f\"  Q-table size: {len(q_ai.q_table)} state-action pairs learned\")\n",
    "    print(f\"  Final epsilon: {q_ai.epsilon:.4f}\")\n",
    "    \n",
    "    # Plot learning progress\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(100, num_games + 1, 100), win_rates)\n",
    "    plt.xlabel(\"Games Played\")\n",
    "    plt.ylabel(\"Win Rate (%)\")\n",
    "    plt.title(\"Q-Learning AI: Win Rate Over Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return q_ai\n",
    "\n",
    "# Train the AI!\n",
    "trained_q_ai = trainQLearning(num_games=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fzpmr0925sr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RLHF AI - Reinforcement Learning from Human Feedback\n",
    "# =============================================================================\n",
    "#\n",
    "# THE KEY DIFFERENCE FROM Q-LEARNING:\n",
    "# Instead of learning from win/loss, we learn from HUMAN PREFERENCES.\n",
    "# We show the human two possible moves and ask: \"Which is better?\"\n",
    "#\n",
    "# HOW IT WORKS:\n",
    "# 1. Given a board state, generate two possible moves\n",
    "# 2. Show both options to the human\n",
    "# 3. Human picks which move they prefer\n",
    "# 4. Update scores: preferred move gets +1, rejected move gets -1\n",
    "#\n",
    "# THIS DEMONSTRATES:\n",
    "# - How modern AI (like ChatGPT) learns from human preferences\n",
    "# - REWARD MISSPECIFICATION: AI learns what humans REWARD, not what they WANT\n",
    "#   Example: If humans always pick \"aggressive\" moves, AI becomes aggressive\n",
    "#            even when defensive play would win the game\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "class RLHF_AI:\n",
    "    def __init__(self, model, playerType, learning_rate=0.3):\n",
    "        self.model = model\n",
    "        self.playerType = playerType\n",
    "        \n",
    "        # PREFERENCE TABLE: like Q-table but learned from human comparisons\n",
    "        # Maps (state, action) → preference score\n",
    "        self.preference_table = {}\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Track training stats\n",
    "        self.comparisons_made = 0\n",
    "    \n",
    "    def boardToString(self, board):\n",
    "        \"\"\"Convert board to string for use as dictionary key\"\"\"\n",
    "        return \"|\".join([\"\".join(row) for row in board])\n",
    "    \n",
    "    def getAvailableMoves(self, board):\n",
    "        \"\"\"Find all empty cells\"\"\"\n",
    "        moves = []\n",
    "        for rowIdx, row in enumerate(board):\n",
    "            for colIdx, cell in enumerate(row):\n",
    "                if cell == \"+\":\n",
    "                    moves.append((rowIdx, colIdx))\n",
    "        return moves\n",
    "    \n",
    "    def getPreference(self, state, action):\n",
    "        \"\"\"Get preference score for a (state, action) pair\"\"\"\n",
    "        return self.preference_table.get((state, action), 0.0)\n",
    "    \n",
    "    def displayBoard(self, board):\n",
    "        \"\"\"Display board state for human review\"\"\"\n",
    "        print(\"    0   1   2\")\n",
    "        print(\"  +---+---+---+\")\n",
    "        for idx, row in enumerate(board):\n",
    "            print(f\"{idx} | {' | '.join(row)} |\")\n",
    "            print(\"  +---+---+---+\")\n",
    "    \n",
    "    def displayComparison(self, board, move_a, move_b):\n",
    "        \"\"\"Show the human two possible moves to compare\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"CURRENT BOARD:\")\n",
    "        self.displayBoard(board)\n",
    "        \n",
    "        print(f\"\\nYou are playing as: {self.playerType}\")\n",
    "        print(\"\\nWhich move is BETTER?\")\n",
    "        print(f\"  [A] Play at position {move_a} (row={move_a[0]}, col={move_a[1]})\")\n",
    "        print(f\"  [B] Play at position {move_b} (row={move_b[0]}, col={move_b[1]})\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    def collectHumanPreference(self, board, move_a, move_b):\n",
    "        \"\"\"\n",
    "        Ask human to compare two moves.\n",
    "        Returns: (preferred_move, rejected_move)\n",
    "        \"\"\"\n",
    "        self.displayComparison(board, move_a, move_b)\n",
    "        \n",
    "        while True:\n",
    "            choice = input(\"Enter A or B (or 'skip' to skip): \").strip().upper()\n",
    "            if choice == 'A':\n",
    "                return (move_a, move_b)\n",
    "            elif choice == 'B':\n",
    "                return (move_b, move_a)\n",
    "            elif choice == 'SKIP':\n",
    "                return (None, None)\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter A, B, or 'skip'\")\n",
    "    \n",
    "    def updateFromComparison(self, state, preferred, rejected):\n",
    "        \"\"\"\n",
    "        Update preference scores based on human choice.\n",
    "        - Preferred move: increase score\n",
    "        - Rejected move: decrease score\n",
    "        \"\"\"\n",
    "        if preferred is None:\n",
    "            return\n",
    "        \n",
    "        # Increase score for preferred move\n",
    "        old_pref = self.getPreference(state, preferred)\n",
    "        self.preference_table[(state, preferred)] = old_pref + self.learning_rate\n",
    "        \n",
    "        # Decrease score for rejected move\n",
    "        old_rej = self.getPreference(state, rejected)\n",
    "        self.preference_table[(state, rejected)] = old_rej - self.learning_rate\n",
    "        \n",
    "        self.comparisons_made += 1\n",
    "    \n",
    "    def train(self, num_comparisons=10):\n",
    "        \"\"\"\n",
    "        Interactive training session.\n",
    "        Shows random board states and asks human to compare moves.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RLHF TRAINING SESSION\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"You will be shown board positions with two possible moves.\")\n",
    "        print(\"Pick which move you think is BETTER.\")\n",
    "        print(\"Your preferences will train the AI!\\n\")\n",
    "        \n",
    "        for i in range(num_comparisons):\n",
    "            print(f\"\\n--- Comparison {i+1}/{num_comparisons} ---\")\n",
    "            \n",
    "            # Generate a random board state (partially filled)\n",
    "            board = [[\"+\" for _ in range(3)] for _ in range(3)]\n",
    "            num_moves = random.randint(0, 5)  # 0-5 moves already made\n",
    "            \n",
    "            pieces = [\"X\", \"O\"]\n",
    "            for j in range(num_moves):\n",
    "                available = self.getAvailableMoves(board)\n",
    "                if available:\n",
    "                    r, c = random.choice(available)\n",
    "                    board[r][c] = pieces[j % 2]\n",
    "            \n",
    "            # Get available moves\n",
    "            available = self.getAvailableMoves(board)\n",
    "            if len(available) < 2:\n",
    "                print(\"(Skipping - not enough moves available)\")\n",
    "                continue\n",
    "            \n",
    "            # Pick two random moves to compare\n",
    "            move_a, move_b = random.sample(available, 2)\n",
    "            \n",
    "            # Get human preference\n",
    "            state = self.boardToString(board)\n",
    "            preferred, rejected = self.collectHumanPreference(board, move_a, move_b)\n",
    "            \n",
    "            # Update from preference\n",
    "            self.updateFromComparison(state, preferred, rejected)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Training complete! Made {self.comparisons_made} comparisons.\")\n",
    "        print(f\"Preference table size: {len(self.preference_table)} entries\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    def getMove(self, model):\n",
    "        \"\"\"\n",
    "        Choose best move according to learned human preferences.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        board = model.board\n",
    "        state = self.boardToString(board)\n",
    "        available = self.getAvailableMoves(board)\n",
    "        \n",
    "        # Find move with highest preference score\n",
    "        best_score = float('-inf')\n",
    "        best_moves = []\n",
    "        \n",
    "        for move in available:\n",
    "            score = self.getPreference(state, move)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_moves = [move]\n",
    "            elif score == best_score:\n",
    "                best_moves.append(move)\n",
    "        \n",
    "        return random.choice(best_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8kjn16up31g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEMO: REWARD MISSPECIFICATION\n",
    "# =============================================================================\n",
    "# This demonstrates how RLHF can go wrong when humans reward the WRONG thing.\n",
    "#\n",
    "# We simulate a \"biased human\" who always prefers:\n",
    "#   - Center moves (1,1)\n",
    "#   - Corner moves (0,0), (0,2), (2,0), (2,2)\n",
    "# ...even when an edge move would be strategically better!\n",
    "#\n",
    "# The AI will learn this bias and play \"stylishly\" instead of optimally.\n",
    "# =============================================================================\n",
    "\n",
    "class BiasedHumanSimulator:\n",
    "    \"\"\"\n",
    "    Simulates a human with a BIAS toward center/corner moves.\n",
    "    This demonstrates reward misspecification - the AI learns our\n",
    "    aesthetic preference, not optimal play.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bias_type=\"center_corner\"):\n",
    "        self.bias_type = bias_type\n",
    "        # Preferred positions (center and corners)\n",
    "        self.preferred = [(1, 1), (0, 0), (0, 2), (2, 0), (2, 2)]\n",
    "    \n",
    "    def choose(self, move_a, move_b):\n",
    "        \"\"\"\n",
    "        Pick between two moves based on bias, not strategy.\n",
    "        Returns: (preferred, rejected)\n",
    "        \"\"\"\n",
    "        a_preferred = move_a in self.preferred\n",
    "        b_preferred = move_b in self.preferred\n",
    "        \n",
    "        if a_preferred and not b_preferred:\n",
    "            return (move_a, move_b)\n",
    "        elif b_preferred and not a_preferred:\n",
    "            return (move_b, move_a)\n",
    "        else:\n",
    "            # Both or neither preferred - pick randomly\n",
    "            if random.random() < 0.5:\n",
    "                return (move_a, move_b)\n",
    "            else:\n",
    "                return (move_b, move_a)\n",
    "\n",
    "\n",
    "def demonstrateRewardMisspecification(num_comparisons=200):\n",
    "    \"\"\"\n",
    "    Train RLHF AI with a biased human simulator.\n",
    "    Then test it against RandomAI to see how the bias affects performance.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"REWARD MISSPECIFICATION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTraining an AI with a BIASED human who prefers center/corners...\")\n",
    "    print(\"The human doesn't consider whether the move actually helps WIN.\\n\")\n",
    "    \n",
    "    # Create AI and biased human\n",
    "    rlhf_ai = RLHF_AI(model=None, playerType=\"X\")\n",
    "    biased_human = BiasedHumanSimulator()\n",
    "    \n",
    "    # Simulate training with biased feedback\n",
    "    for i in range(num_comparisons):\n",
    "        # Generate random board state\n",
    "        board = [[\"+\" for _ in range(3)] for _ in range(3)]\n",
    "        num_moves = random.randint(0, 5)\n",
    "        \n",
    "        pieces = [\"X\", \"O\"]\n",
    "        for j in range(num_moves):\n",
    "            available = rlhf_ai.getAvailableMoves(board)\n",
    "            if available:\n",
    "                r, c = random.choice(available)\n",
    "                board[r][c] = pieces[j % 2]\n",
    "        \n",
    "        available = rlhf_ai.getAvailableMoves(board)\n",
    "        if len(available) < 2:\n",
    "            continue\n",
    "        \n",
    "        move_a, move_b = random.sample(available, 2)\n",
    "        state = rlhf_ai.boardToString(board)\n",
    "        \n",
    "        # Get BIASED human preference (not optimal!)\n",
    "        preferred, rejected = biased_human.choose(move_a, move_b)\n",
    "        rlhf_ai.updateFromComparison(state, preferred, rejected)\n",
    "    \n",
    "    print(f\"Training complete! {rlhf_ai.comparisons_made} comparisons made.\\n\")\n",
    "    \n",
    "    # Show what the AI learned\n",
    "    print(\"What the AI learned (preference scores):\")\n",
    "    print(\"  Center (1,1): HIGHLY preferred\")\n",
    "    print(\"  Corners: preferred\")\n",
    "    print(\"  Edges: NOT preferred\")\n",
    "    print(\"\\nThis is REWARD MISSPECIFICATION - the AI learned our style,\")\n",
    "    print(\"not how to win!\\n\")\n",
    "    \n",
    "    # Test against RandomAI\n",
    "    print(\"Testing biased RLHF AI vs RandomAI (500 games)...\")\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    ties = 0\n",
    "    \n",
    "    for _ in range(500):\n",
    "        board = [[\"+\" for _ in range(3)] for _ in range(3)]\n",
    "        playerX = Player(\"X\")\n",
    "        playerO = Player(\"O\")\n",
    "        model = Model(board, \"X\", playerX, playerO)\n",
    "        opponent = RandomAI(model=None, playerType=\"O\")\n",
    "        \n",
    "        while not model.gameOver:\n",
    "            if model.turn == \"X\":\n",
    "                x, y = rlhf_ai.getMove(model)\n",
    "            else:\n",
    "                x, y = opponent.getMove(model)\n",
    "            \n",
    "            model.makeMove(x, y)\n",
    "            model.checkGameOver()\n",
    "            if not model.gameOver:\n",
    "                model.turn = \"O\" if model.turn == \"X\" else \"X\"\n",
    "        \n",
    "        if model.winner == \"X\":\n",
    "            wins += 1\n",
    "        elif model.winner == \"O\":\n",
    "            losses += 1\n",
    "        else:\n",
    "            ties += 1\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Wins:   {wins} ({wins/5:.1f}%)\")\n",
    "    print(f\"  Losses: {losses} ({losses/5:.1f}%)\")\n",
    "    print(f\"  Ties:   {ties} ({ties/5:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY INSIGHT:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"The AI plays 'stylishly' (center/corners) but not OPTIMALLY.\")\n",
    "    print(\"It learned what we REWARDED, not what we actually WANTED (winning).\")\n",
    "    print(\"This is why specifying rewards correctly is so important in AI!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return rlhf_ai\n",
    "\n",
    "# Run the demo!\n",
    "biased_rlhf_ai = demonstrateRewardMisspecification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rn6dd8a2w4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARING MOVE DISTRIBUTIONS: Q-Learning vs RLHF\n",
    "# =============================================================================\n",
    "# This visualizes WHERE each AI likes to play, showing how biased human\n",
    "# feedback changes the AI's behavior compared to learning from wins/losses.\n",
    "# =============================================================================\n",
    "\n",
    "def compareMoveDitributions(num_training_games=3000, num_test_games=500):\n",
    "    \"\"\"\n",
    "    Train both Q-Learning and RLHF (with biased human), then compare\n",
    "    which positions they prefer to play.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"MOVE DISTRIBUTION COMPARISON: Q-Learning vs Biased RLHF\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. Train Q-Learning AI (learns from wins/losses)\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[1/4] Training Q-Learning AI (learns from game outcomes)...\")\n",
    "    \n",
    "    q_ai = QLearningAI(model=None, playerType=\"X\", epsilon=0.5, learning_rate=0.5)\n",
    "    \n",
    "    for _ in range(num_training_games):\n",
    "        board = [[\"+\" for _ in range(3)] for _ in range(3)]\n",
    "        playerX = Player(\"X\")\n",
    "        playerO = Player(\"O\")\n",
    "        model = Model(board, \"X\", playerX, playerO)\n",
    "        opponent = RandomAI(model=None, playerType=\"O\")\n",
    "        \n",
    "        while not model.gameOver:\n",
    "            if model.turn == \"X\":\n",
    "                x, y = q_ai.getMove(model)\n",
    "            else:\n",
    "                x, y = opponent.getMove(model)\n",
    "            model.makeMove(x, y)\n",
    "            model.checkGameOver()\n",
    "            if not model.gameOver:\n",
    "                model.turn = \"O\" if model.turn == \"X\" else \"X\"\n",
    "        \n",
    "        if model.winner == \"X\":\n",
    "            q_ai.learn(reward=1.0)\n",
    "        elif model.winner == \"O\":\n",
    "            q_ai.learn(reward=-1.0)\n",
    "        else:\n",
    "            q_ai.learn(reward=0.5)\n",
    "        q_ai.decayEpsilon(decay_rate=0.999)\n",
    "    \n",
    "    q_ai.epsilon = 0  # No exploration during testing\n",
    "    print(f\"   Q-Learning trained! Q-table size: {len(q_ai.q_table)}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. Train RLHF AI (learns from biased human preferences)\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[2/4] Training RLHF AI (learns from biased human who loves center/corners)...\")\n",
    "    \n",
    "    rlhf_ai = RLHF_AI(model=None, playerType=\"X\", learning_rate=0.3)\n",
    "    biased_human = BiasedHumanSimulator()\n",
    "    \n",
    "    for _ in range(500):  # 500 comparisons\n",
    "        board = [[\"+\" for _ in range(3)] for _ in range(3)]\n",
    "        num_moves = random.randint(0, 5)\n",
    "        pieces = [\"X\", \"O\"]\n",
    "        for j in range(num_moves):\n",
    "            available = rlhf_ai.getAvailableMoves(board)\n",
    "            if available:\n",
    "                r, c = random.choice(available)\n",
    "                board[r][c] = pieces[j % 2]\n",
    "        \n",
    "        available = rlhf_ai.getAvailableMoves(board)\n",
    "        if len(available) < 2:\n",
    "            continue\n",
    "        \n",
    "        move_a, move_b = random.sample(available, 2)\n",
    "        state = rlhf_ai.boardToString(board)\n",
    "        preferred, rejected = biased_human.choose(move_a, move_b)\n",
    "        rlhf_ai.updateFromComparison(state, preferred, rejected)\n",
    "    \n",
    "    print(f\"   RLHF trained! {rlhf_ai.comparisons_made} comparisons made.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. Collect move distributions from test games\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"\\n[3/4] Playing {num_test_games} test games with each AI...\")\n",
    "    \n",
    "    # Track moves for each AI: 3x3 grid of counts\n",
    "    q_moves = np.zeros((3, 3))\n",
    "    rlhf_moves = np.zeros((3, 3))\n",
    "    \n",
    "    # Q-Learning test games\n",
    "    q_wins, q_losses, q_ties = 0, 0, 0\n",
    "    for _ in range(num_test_games):\n",
    "        board = [[\"+\" for _ in range(3)] for _ in range(3)]\n",
    "        playerX = Player(\"X\")\n",
    "        playerO = Player(\"O\")\n",
    "        model = Model(board, \"X\", playerX, playerO)\n",
    "        opponent = RandomAI(model=None, playerType=\"O\")\n",
    "        \n",
    "        while not model.gameOver:\n",
    "            if model.turn == \"X\":\n",
    "                x, y = q_ai.getMove(model)\n",
    "                q_moves[x][y] += 1  # Track the move\n",
    "            else:\n",
    "                x, y = opponent.getMove(model)\n",
    "            model.makeMove(x, y)\n",
    "            model.checkGameOver()\n",
    "            if not model.gameOver:\n",
    "                model.turn = \"O\" if model.turn == \"X\" else \"X\"\n",
    "        \n",
    "        if model.winner == \"X\":\n",
    "            q_wins += 1\n",
    "        elif model.winner == \"O\":\n",
    "            q_losses += 1\n",
    "        else:\n",
    "            q_ties += 1\n",
    "    \n",
    "    # RLHF test games\n",
    "    rlhf_wins, rlhf_losses, rlhf_ties = 0, 0, 0\n",
    "    for _ in range(num_test_games):\n",
    "        board = [[\"+\" for _ in range(3)] for _ in range(3)]\n",
    "        playerX = Player(\"X\")\n",
    "        playerO = Player(\"O\")\n",
    "        model = Model(board, \"X\", playerX, playerO)\n",
    "        opponent = RandomAI(model=None, playerType=\"O\")\n",
    "        \n",
    "        while not model.gameOver:\n",
    "            if model.turn == \"X\":\n",
    "                x, y = rlhf_ai.getMove(model)\n",
    "                rlhf_moves[x][y] += 1  # Track the move\n",
    "            else:\n",
    "                x, y = opponent.getMove(model)\n",
    "            model.makeMove(x, y)\n",
    "            model.checkGameOver()\n",
    "            if not model.gameOver:\n",
    "                model.turn = \"O\" if model.turn == \"X\" else \"X\"\n",
    "        \n",
    "        if model.winner == \"X\":\n",
    "            rlhf_wins += 1\n",
    "        elif model.winner == \"O\":\n",
    "            rlhf_losses += 1\n",
    "        else:\n",
    "            rlhf_ties += 1\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. Visualize the results\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[4/4] Creating visualizations...\")\n",
    "    \n",
    "    # Normalize to percentages\n",
    "    q_moves_pct = q_moves / q_moves.sum() * 100\n",
    "    rlhf_moves_pct = rlhf_moves / rlhf_moves.sum() * 100\n",
    "    \n",
    "    # Create figure with multiple plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Position labels for the board\n",
    "    position_labels = [\n",
    "        [\"Corner\\n(0,0)\", \"Edge\\n(0,1)\", \"Corner\\n(0,2)\"],\n",
    "        [\"Edge\\n(1,0)\", \"CENTER\\n(1,1)\", \"Edge\\n(1,2)\"],\n",
    "        [\"Corner\\n(2,0)\", \"Edge\\n(2,1)\", \"Corner\\n(2,2)\"]\n",
    "    ]\n",
    "    \n",
    "    # Plot 1: Q-Learning heatmap\n",
    "    im1 = axes[0].imshow(q_moves_pct, cmap='Blues', vmin=0, vmax=max(q_moves_pct.max(), rlhf_moves_pct.max()))\n",
    "    axes[0].set_title(f'Q-Learning Move Distribution\\nWins: {q_wins/num_test_games*100:.1f}%', fontsize=12)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            axes[0].text(j, i, f'{q_moves_pct[i,j]:.1f}%', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks([0, 1, 2])\n",
    "    axes[0].set_yticks([0, 1, 2])\n",
    "    axes[0].set_xticklabels(['Col 0', 'Col 1', 'Col 2'])\n",
    "    axes[0].set_yticklabels(['Row 0', 'Row 1', 'Row 2'])\n",
    "    \n",
    "    # Plot 2: RLHF heatmap\n",
    "    im2 = axes[1].imshow(rlhf_moves_pct, cmap='Reds', vmin=0, vmax=max(q_moves_pct.max(), rlhf_moves_pct.max()))\n",
    "    axes[1].set_title(f'RLHF (Biased) Move Distribution\\nWins: {rlhf_wins/num_test_games*100:.1f}%', fontsize=12)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            axes[1].text(j, i, f'{rlhf_moves_pct[i,j]:.1f}%', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks([0, 1, 2])\n",
    "    axes[1].set_yticks([0, 1, 2])\n",
    "    axes[1].set_xticklabels(['Col 0', 'Col 1', 'Col 2'])\n",
    "    axes[1].set_yticklabels(['Row 0', 'Row 1', 'Row 2'])\n",
    "    \n",
    "    # Plot 3: Bar chart comparison\n",
    "    positions = ['Corners\\n(0,0)(0,2)\\n(2,0)(2,2)', 'Edges\\n(0,1)(1,0)\\n(1,2)(2,1)', 'Center\\n(1,1)']\n",
    "    \n",
    "    # Calculate totals for each category\n",
    "    q_corners = q_moves_pct[0,0] + q_moves_pct[0,2] + q_moves_pct[2,0] + q_moves_pct[2,2]\n",
    "    q_edges = q_moves_pct[0,1] + q_moves_pct[1,0] + q_moves_pct[1,2] + q_moves_pct[2,1]\n",
    "    q_center = q_moves_pct[1,1]\n",
    "    \n",
    "    rlhf_corners = rlhf_moves_pct[0,0] + rlhf_moves_pct[0,2] + rlhf_moves_pct[2,0] + rlhf_moves_pct[2,2]\n",
    "    rlhf_edges = rlhf_moves_pct[0,1] + rlhf_moves_pct[1,0] + rlhf_moves_pct[1,2] + rlhf_moves_pct[2,1]\n",
    "    rlhf_center = rlhf_moves_pct[1,1]\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[2].bar(x - width/2, [q_corners, q_edges, q_center], width, label='Q-Learning', color='steelblue')\n",
    "    bars2 = axes[2].bar(x + width/2, [rlhf_corners, rlhf_edges, rlhf_center], width, label='RLHF (Biased)', color='indianred')\n",
    "    \n",
    "    axes[2].set_ylabel('% of Total Moves')\n",
    "    axes[2].set_title('Move Type Comparison', fontsize=12)\n",
    "    axes[2].set_xticks(x)\n",
    "    axes[2].set_xticklabels(positions)\n",
    "    axes[2].legend()\n",
    "    axes[2].set_ylim(0, 60)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Metric':<25} {'Q-Learning':>15} {'RLHF (Biased)':>15}\")\n",
    "    print(\"-\"*55)\n",
    "    print(f\"{'Win Rate':<25} {q_wins/num_test_games*100:>14.1f}% {rlhf_wins/num_test_games*100:>14.1f}%\")\n",
    "    print(f\"{'Center moves':<25} {q_center:>14.1f}% {rlhf_center:>14.1f}%\")\n",
    "    print(f\"{'Corner moves':<25} {q_corners:>14.1f}% {rlhf_corners:>14.1f}%\")\n",
    "    print(f\"{'Edge moves':<25} {q_edges:>14.1f}% {rlhf_edges:>14.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY INSIGHT\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "    Q-Learning learned from WINS → plays strategically\n",
    "    RLHF learned from BIASED HUMAN → plays \"stylishly\" (center/corners)\n",
    "    \n",
    "    Notice how RLHF has:\n",
    "    - HIGHER center/corner usage (what the biased human rewarded)\n",
    "    - LOWER edge usage (what the biased human ignored)\n",
    "    - LOWER win rate (because style ≠ strategy)\n",
    "    \n",
    "    The AI perfectly learned what we taught it...\n",
    "    but we taught it the WRONG thing!\n",
    "    \"\"\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return q_ai, rlhf_ai\n",
    "\n",
    "# Run the comparison!\n",
    "q_ai_trained, rlhf_ai_trained = compareMoveDitributions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
