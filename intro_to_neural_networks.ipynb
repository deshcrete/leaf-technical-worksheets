{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks: Building a Tiny Language Model\n",
    "\n",
    "In this notebook, we'll build a simple neural network that learns to predict the next character in text. We'll use Shakespeare's writing as our training data!\n",
    "\n",
    "## What We'll Learn:\n",
    "1. **One-Hot Encoding** - How to turn letters into numbers\n",
    "2. **Attention** - How the model \"pays attention\" to different parts of the input\n",
    "3. **MLP (Multi-Layer Perceptron)** - A simple neural network layer\n",
    "4. **Training** - How the model learns from examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get the Data\n",
    "\n",
    "Let's download a small piece of Shakespeare's writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download tiny shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "urllib.request.urlretrieve(url, \"shakespeare.txt\")\n",
    "\n",
    "# Read the text\n",
    "with open(\"shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"\\nFirst 500 characters:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Vocabulary\n",
    "\n",
    "We need to know all the unique characters in our text. This is our \"vocabulary\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "# Create mappings: character <-> number\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"\\nExample: 'a' -> {char_to_idx['a']}\")\n",
    "print(f\"Example: {char_to_idx['a']} -> '{idx_to_char[char_to_idx['a']]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: One-Hot Encoding\n",
    "\n",
    "Neural networks work with numbers, not letters. **One-hot encoding** converts each character into a vector of 0s with a single 1.\n",
    "\n",
    "For example, if we have 3 characters [a, b, c]:\n",
    "- 'a' becomes [1, 0, 0]\n",
    "- 'b' becomes [0, 1, 0]\n",
    "- 'c' becomes [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(indices, vocab_size):\n",
    "    \"\"\"\n",
    "    Convert character indices to one-hot vectors.\n",
    "    \n",
    "    Args:\n",
    "        indices: list of character indices, shape (sequence_length,)\n",
    "        vocab_size: total number of unique characters\n",
    "    \n",
    "    Returns:\n",
    "        one_hot: array of shape (sequence_length, vocab_size)\n",
    "    \"\"\"\n",
    "    seq_len = len(indices)\n",
    "    one_hot = np.zeros((seq_len, vocab_size))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        one_hot[i, idx] = 1.0\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "# Example\n",
    "example_text = \"hello\"\n",
    "example_indices = [char_to_idx[c] for c in example_text]\n",
    "example_one_hot = one_hot_encode(example_indices, vocab_size)\n",
    "\n",
    "print(f\"Text: '{example_text}'\")\n",
    "print(f\"Indices: {example_indices}\")\n",
    "print(f\"One-hot shape: {example_one_hot.shape}\")\n",
    "print(f\"\\nOne-hot for 'h' (index {char_to_idx['h']}):\")\n",
    "print(f\"Position of the 1: {np.argmax(example_one_hot[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data\n",
    "\n",
    "We'll train the model to predict the next character. Given \"hell\", predict \"o\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller subset for faster training\n",
    "text = text[:10000]\n",
    "\n",
    "# Convert entire text to indices\n",
    "data = [char_to_idx[c] for c in text]\n",
    "\n",
    "# Context length: how many characters the model sees to predict the next one\n",
    "context_length = 8\n",
    "\n",
    "def get_batch(data, batch_size, context_length):\n",
    "    \"\"\"\n",
    "    Get a random batch of training examples.\n",
    "    \n",
    "    Returns:\n",
    "        X: input sequences, shape (batch_size, context_length, vocab_size)\n",
    "        Y: target characters (what comes next), shape (batch_size,)\n",
    "    \"\"\"\n",
    "    # Pick random starting positions\n",
    "    starts = np.random.randint(0, len(data) - context_length - 1, batch_size)\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for start in starts:\n",
    "        # Input: context_length characters\n",
    "        input_indices = data[start:start + context_length]\n",
    "        # Target: the next character\n",
    "        target = data[start + context_length]\n",
    "        \n",
    "        X.append(one_hot_encode(input_indices, vocab_size))\n",
    "        Y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Test it\n",
    "X_test, Y_test = get_batch(data, batch_size=2, context_length=context_length)\n",
    "print(f\"Input shape: {X_test.shape}\")  # (batch, context_length, vocab_size)\n",
    "print(f\"Target shape: {Y_test.shape}\")  # (batch,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: The Attention Mechanism (Simplified)\n",
    "\n",
    "**Attention** lets the model decide which parts of the input are most important for making a prediction.\n",
    "\n",
    "The idea:\n",
    "1. Each position asks: \"How relevant is every other position to me?\"\n",
    "2. We compute **attention scores** between all positions\n",
    "3. We use these scores to create a weighted combination of the input\n",
    "\n",
    "Think of it like reading a sentence and highlighting the most important words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Convert scores to probabilities (0 to 1, sum to 1).\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "class SimpleAttention:\n",
    "    \"\"\"\n",
    "    A barebones attention layer.\n",
    "    \n",
    "    For each position, it computes how much to \"attend\" to every other position,\n",
    "    then creates a weighted sum of all positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        # Initialize weights randomly (small values)\n",
    "        scale = 0.1\n",
    "        \n",
    "        # Query: \"What am I looking for?\"\n",
    "        self.W_query = np.random.randn(input_dim, hidden_dim) * scale\n",
    "        \n",
    "        # Key: \"What do I contain?\"\n",
    "        self.W_key = np.random.randn(input_dim, hidden_dim) * scale\n",
    "        \n",
    "        # Value: \"What information do I provide?\"\n",
    "        self.W_value = np.random.randn(input_dim, hidden_dim) * scale\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input of shape (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: shape (batch_size, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Store input for backward pass\n",
    "        self.x = x\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Compute Query, Key, Value\n",
    "        # Each position gets its own Q, K, V vector\n",
    "        self.Q = x @ self.W_query  # (batch, seq_len, hidden_dim)\n",
    "        self.K = x @ self.W_key    # (batch, seq_len, hidden_dim)\n",
    "        self.V = x @ self.W_value  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Compute attention scores: how much should position i attend to position j?\n",
    "        # Score = Q @ K^T\n",
    "        # Shape: (batch, seq_len, seq_len)\n",
    "        scores = self.Q @ self.K.transpose(0, 2, 1)\n",
    "        \n",
    "        # Scale scores (helps with training stability)\n",
    "        hidden_dim = self.W_query.shape[1]\n",
    "        scores = scores / np.sqrt(hidden_dim)\n",
    "        \n",
    "        # Apply causal mask: can only attend to previous positions!\n",
    "        # This is crucial for language modeling - we can't peek at the future\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1) * -1e9\n",
    "        scores = scores + mask\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        self.attention_weights = softmax(scores)  # (batch, seq_len, seq_len)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        output = self.attention_weights @ self.V  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        \"\"\"\n",
    "        Compute gradients and update weights.\n",
    "        This is a simplified backward pass.\n",
    "        \"\"\"\n",
    "        batch_size = self.x.shape[0]\n",
    "        \n",
    "        # Gradient for V: attention_weights^T @ grad_output\n",
    "        grad_V = self.attention_weights.transpose(0, 2, 1) @ grad_output\n",
    "        \n",
    "        # Gradient for W_value\n",
    "        grad_W_value = np.zeros_like(self.W_value)\n",
    "        for b in range(batch_size):\n",
    "            grad_W_value += self.x[b].T @ grad_V[b]\n",
    "        grad_W_value /= batch_size\n",
    "        \n",
    "        # Simplified gradients for Q and K (approximate)\n",
    "        grad_W_query = np.zeros_like(self.W_query)\n",
    "        grad_W_key = np.zeros_like(self.W_key)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # This is a simplified approximation\n",
    "            grad_attn = grad_output[b] @ self.V[b].T\n",
    "            grad_Q = grad_attn @ self.K[b]\n",
    "            grad_K = grad_attn.T @ self.Q[b]\n",
    "            grad_W_query += self.x[b].T @ grad_Q\n",
    "            grad_W_key += self.x[b].T @ grad_K\n",
    "        \n",
    "        grad_W_query /= batch_size\n",
    "        grad_W_key /= batch_size\n",
    "        \n",
    "        # Update weights\n",
    "        self.W_query -= learning_rate * grad_W_query\n",
    "        self.W_key -= learning_rate * grad_W_key\n",
    "        self.W_value -= learning_rate * grad_W_value\n",
    "        \n",
    "        # Return gradient for previous layer\n",
    "        grad_x = grad_V @ self.W_value.T\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "# Test the attention layer\n",
    "attn = SimpleAttention(input_dim=vocab_size, hidden_dim=32)\n",
    "test_output = attn.forward(X_test)\n",
    "print(f\"Attention input shape: {X_test.shape}\")\n",
    "print(f\"Attention output shape: {test_output.shape}\")\n",
    "print(f\"Attention weights shape: {attn.attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: The MLP Layer\n",
    "\n",
    "An **MLP (Multi-Layer Perceptron)** is the simplest type of neural network layer:\n",
    "1. Multiply input by weights\n",
    "2. Add a bias\n",
    "3. Apply an activation function (we'll use ReLU: max(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"ReLU activation: keep positive values, set negative to 0.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_backward(x):\n",
    "    \"\"\"Gradient of ReLU: 1 if x > 0, else 0.\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    A simple two-layer MLP.\n",
    "    \n",
    "    input -> hidden layer -> ReLU -> output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        scale = 0.1\n",
    "        \n",
    "        # First layer: input -> hidden\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * scale\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        \n",
    "        # Second layer: hidden -> output\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * scale\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input of shape (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        \n",
    "        # First layer\n",
    "        self.z1 = x @ self.W1 + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        \n",
    "        # Second layer (no activation - we'll apply softmax later)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        \n",
    "        return self.z2\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        \"\"\"\n",
    "        Backpropagation: compute gradients and update weights.\n",
    "        \"\"\"\n",
    "        batch_size = self.x.shape[0]\n",
    "        \n",
    "        # Gradient for second layer\n",
    "        grad_W2 = self.a1.T @ grad_output / batch_size\n",
    "        grad_b2 = np.mean(grad_output, axis=0)\n",
    "        \n",
    "        # Gradient through second layer\n",
    "        grad_a1 = grad_output @ self.W2.T\n",
    "        \n",
    "        # Gradient through ReLU\n",
    "        grad_z1 = grad_a1 * relu_backward(self.z1)\n",
    "        \n",
    "        # Gradient for first layer\n",
    "        grad_W1 = self.x.T @ grad_z1 / batch_size\n",
    "        grad_b1 = np.mean(grad_z1, axis=0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * grad_W2\n",
    "        self.b2 -= learning_rate * grad_b2\n",
    "        self.W1 -= learning_rate * grad_W1\n",
    "        self.b1 -= learning_rate * grad_b1\n",
    "        \n",
    "        # Return gradient for previous layer\n",
    "        return grad_z1 @ self.W1.T\n",
    "\n",
    "\n",
    "# Test the MLP\n",
    "mlp = MLP(input_dim=32, hidden_dim=64, output_dim=vocab_size)\n",
    "# We need to flatten the attention output: take the last position\n",
    "mlp_input = test_output[:, -1, :]  # (batch, hidden_dim)\n",
    "mlp_output = mlp.forward(mlp_input)\n",
    "print(f\"MLP input shape: {mlp_input.shape}\")\n",
    "print(f\"MLP output shape: {mlp_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Loss Function\n",
    "\n",
    "We need a way to measure how wrong our predictions are. We use **cross-entropy loss**:\n",
    "- The model outputs probabilities for each character\n",
    "- We want high probability for the correct character\n",
    "- Loss = -log(probability of correct character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        logits: model outputs, shape (batch_size, vocab_size)\n",
    "        targets: correct character indices, shape (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        loss: scalar\n",
    "        probs: probabilities, shape (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    # Convert logits to probabilities\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    # Get probability of correct character for each example\n",
    "    batch_size = logits.shape[0]\n",
    "    correct_probs = probs[np.arange(batch_size), targets]\n",
    "    \n",
    "    # Loss = -log(correct probability)\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    loss = -np.mean(np.log(correct_probs + 1e-9))\n",
    "    \n",
    "    return loss, probs\n",
    "\n",
    "\n",
    "def cross_entropy_backward(probs, targets):\n",
    "    \"\"\"\n",
    "    Gradient of cross-entropy loss.\n",
    "    \n",
    "    The gradient is simply: probs - one_hot(targets)\n",
    "    \"\"\"\n",
    "    batch_size = probs.shape[0]\n",
    "    grad = probs.copy()\n",
    "    grad[np.arange(batch_size), targets] -= 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Put It All Together - The Model\n",
    "\n",
    "Our complete model:\n",
    "1. Input: one-hot encoded characters\n",
    "2. Attention layer: learns which characters are important\n",
    "3. MLP: makes the final prediction\n",
    "4. Output: probability for each possible next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel:\n",
    "    \"\"\"\n",
    "    A tiny language model: Attention + MLP\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim=32, mlp_hidden=64):\n",
    "        self.attention = SimpleAttention(vocab_size, hidden_dim)\n",
    "        self.mlp = MLP(hidden_dim, mlp_hidden, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: one-hot input, shape (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        Returns:\n",
    "            logits: shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Attention layer\n",
    "        attn_out = self.attention.forward(x)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Take the last position (we're predicting what comes next)\n",
    "        last_hidden = attn_out[:, -1, :]  # (batch, hidden_dim)\n",
    "        \n",
    "        # MLP to get predictions\n",
    "        logits = self.mlp.forward(last_hidden)  # (batch, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        \"\"\"\n",
    "        Backpropagate through the model.\n",
    "        \"\"\"\n",
    "        # Backward through MLP\n",
    "        grad_last_hidden = self.mlp.backward(grad_output, learning_rate)\n",
    "        \n",
    "        # Expand gradient to full sequence (only last position has gradient)\n",
    "        batch_size = grad_last_hidden.shape[0]\n",
    "        seq_len = self.attention.x.shape[1]\n",
    "        hidden_dim = grad_last_hidden.shape[1]\n",
    "        \n",
    "        grad_attn_out = np.zeros((batch_size, seq_len, hidden_dim))\n",
    "        grad_attn_out[:, -1, :] = grad_last_hidden\n",
    "        \n",
    "        # Backward through attention\n",
    "        self.attention.backward(grad_attn_out, learning_rate)\n",
    "    \n",
    "    def generate(self, start_text, length, char_to_idx, idx_to_char, context_length):\n",
    "        \"\"\"\n",
    "        Generate new text starting from start_text.\n",
    "        \"\"\"\n",
    "        # Convert start text to indices\n",
    "        current = [char_to_idx[c] for c in start_text]\n",
    "        \n",
    "        generated = start_text\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Get the last context_length characters\n",
    "            context = current[-context_length:]\n",
    "            \n",
    "            # Pad if needed\n",
    "            while len(context) < context_length:\n",
    "                context = [0] + context\n",
    "            \n",
    "            # One-hot encode\n",
    "            x = one_hot_encode(context, self.vocab_size)\n",
    "            x = x[np.newaxis, :, :]  # Add batch dimension\n",
    "            \n",
    "            # Get prediction\n",
    "            logits = self.forward(x)\n",
    "            probs = softmax(logits[0])\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            next_idx = np.random.choice(len(probs), p=probs)\n",
    "            \n",
    "            # Add to sequence\n",
    "            current.append(next_idx)\n",
    "            generated += idx_to_char[next_idx]\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = SimpleLanguageModel(vocab_size, hidden_dim=32, mlp_hidden=64)\n",
    "print(\"Model created!\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Training!\n",
    "\n",
    "Now we train the model:\n",
    "1. Get a batch of examples\n",
    "2. Forward pass: compute predictions\n",
    "3. Compute loss\n",
    "4. Backward pass: compute gradients\n",
    "5. Update weights\n",
    "6. Repeat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "num_steps = 1000\n",
    "\n",
    "# Track losses\n",
    "losses = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Get a batch\n",
    "    X_batch, Y_batch = get_batch(data, batch_size, context_length)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model.forward(X_batch)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss, probs = cross_entropy_loss(logits, Y_batch)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad = cross_entropy_backward(probs, Y_batch)\n",
    "    model.backward(grad, learning_rate)\n",
    "    \n",
    "    # Print progress\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step:4d} | Loss: {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Starting loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Generate Text!\n",
    "\n",
    "Now let's see what our model learned! We'll give it a starting prompt and let it generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text\n",
    "print(\"Generated text samples:\\n\")\n",
    "\n",
    "prompts = [\"The \", \"KING\", \"What \"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = model.generate(\n",
    "        start_text=prompt,\n",
    "        length=100,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char,\n",
    "        context_length=context_length\n",
    "    )\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Visualize Attention\n",
    "\n",
    "Let's see what the model is \"paying attention\" to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for a specific input\n",
    "test_text = \"to be or\"\n",
    "test_indices = [char_to_idx[c] for c in test_text]\n",
    "test_one_hot = one_hot_encode(test_indices, vocab_size)\n",
    "test_input = test_one_hot[np.newaxis, :, :]  # Add batch dimension\n",
    "\n",
    "# Forward pass\n",
    "_ = model.forward(test_input)\n",
    "\n",
    "# Get attention weights\n",
    "attn_weights = model.attention.attention_weights[0]  # (seq_len, seq_len)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(attn_weights, cmap='Blues')\n",
    "\n",
    "# Labels\n",
    "chars_list = list(test_text)\n",
    "ax.set_xticks(range(len(chars_list)))\n",
    "ax.set_yticks(range(len(chars_list)))\n",
    "ax.set_xticklabels(chars_list)\n",
    "ax.set_yticklabels(chars_list)\n",
    "\n",
    "ax.set_xlabel('Key (attending TO)')\n",
    "ax.set_ylabel('Query (attending FROM)')\n",
    "ax.set_title(f'Attention Pattern for \"{test_text}\"')\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Each row shows how much that position attends to previous positions.\")\n",
    "print(\"The diagonal pattern shows the causal mask - can only attend to past!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We built a tiny language model from scratch! Here's what we learned:\n",
    "\n",
    "1. **One-Hot Encoding**: Convert characters to vectors that neural networks can understand\n",
    "\n",
    "2. **Attention**: A mechanism that lets the model decide which parts of the input are most relevant. Uses Query (what am I looking for?), Key (what do I contain?), and Value (what information do I provide?)\n",
    "\n",
    "3. **MLP**: A simple neural network layer that transforms data through weights, biases, and activation functions\n",
    "\n",
    "4. **Training**: Use loss functions and backpropagation to teach the model by showing it examples\n",
    "\n",
    "### What Makes Real Models (like GPT) Different?\n",
    "- Much larger (billions of parameters vs our ~10,000)\n",
    "- Multiple attention layers stacked\n",
    "- Better embeddings (learned, not one-hot)\n",
    "- More training data and compute\n",
    "- Advanced techniques (layer norm, dropout, etc.)\n",
    "\n",
    "But the core ideas are the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "def count_params(model):\n",
    "    total = 0\n",
    "    # Attention\n",
    "    total += model.attention.W_query.size\n",
    "    total += model.attention.W_key.size\n",
    "    total += model.attention.W_value.size\n",
    "    # MLP\n",
    "    total += model.mlp.W1.size + model.mlp.b1.size\n",
    "    total += model.mlp.W2.size + model.mlp.b2.size\n",
    "    return total\n",
    "\n",
    "print(f\"Our model has {count_params(model):,} parameters\")\n",
    "print(f\"GPT-3 has 175,000,000,000 parameters\")\n",
    "print(f\"That's {175_000_000_000 / count_params(model):,.0f}x more!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
