{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'leaf-technical-worksheets'...\n"
     ]
    }
   ],
   "source": [
    "!git clone \"https://github.com/deshcrete/leaf-technical-worksheets.git\"\n",
    "!mv leaf-technical-worksheets/* .\n",
    "!mv week-1-Interpreting-NNs/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urPtQG8OZQOy"
   },
   "outputs": [],
   "source": [
    "# Import helper functions - all the implementation details are hidden here!\n",
    "from helper_interp import (\n",
    "    # Model and training\n",
    "    create_model, create_optimizer, train_model, \n",
    "    load_data, create_dataloaders, plot_training_history,\n",
    "    # Visualization\n",
    "    visualize_network_architecture, visualize_sample_digits, visualize_prediction,\n",
    "    # Activations\n",
    "    plot_activations, get_sample_by_digit, get_sample_by_index,\n",
    "    # Probes\n",
    "    train_probe, visualize_probe_weights, compare_probes_across_layers\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "Our MNIST classifier has the following structure:\n",
    "- **Input**: 784 neurons (28x28 flattened image)\n",
    "- **Hidden Layer 1**: 32 neurons + ReLU\n",
    "- **Hidden Layer 2**: 16 neurons + ReLU\n",
    "- **Output**: 10 neurons (one per digit 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the network architecture\n",
    "visualize_network_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9gWr7eva9ue"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST data\n",
    "train_imgs, train_lbls, test_imgs, test_lbls = load_data(\n",
    "    '/content/sample_data/mnist_train_small.csv', \n",
    "    '/content/sample_data/mnist_test.csv'\n",
    ")\n",
    "train_loader, test_loader = create_dataloaders(train_imgs, train_lbls, test_imgs, test_lbls)\n",
    "\n",
    "print(f\"Loaded {len(train_imgs)} training samples and {len(test_imgs)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Input Digits\n",
    "\n",
    "Let's see what MNIST digits look like - these 28x28 grayscale images are flattened to 784 values before entering the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample digits (one of each 0-9)\n",
    "visualize_sample_digits(train_imgs, train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJju48Z4brO5",
    "outputId": "6f0898eb-3c43-4f1f-854b-f7c02e7aa1d0"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING PARAMETERS - You can modify these!\n",
    "# =============================================================================\n",
    "EPOCHS = 20           # Number of training epochs\n",
    "LEARNING_RATE = 0.001  # Learning rate for optimizer\n",
    "\n",
    "# Create and train the model\n",
    "model = create_model()\n",
    "optimizer = create_optimizer(model, lr=LEARNING_RATE)\n",
    "history = train_model(model, optimizer, train_loader, test_loader, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "Xx6N-YDicPmf",
    "outputId": "c342d57a-6f4e-4c4a-dce6-d9dc4181fbb4"
   },
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Output Distributions\n",
    "\n",
    "The network outputs 10 logits which are converted to probabilities via softmax. Let's see how the model's predictions look for different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHOOSE A DIGIT TO VISUALIZE - Change this value!\n",
    "# =============================================================================\n",
    "DIGIT_TO_VIEW = 7  # Try values 0-9\n",
    "\n",
    "# Get a sample of that digit and show its prediction\n",
    "image, label = get_sample_by_digit(test_imgs, test_lbls, DIGIT_TO_VIEW)\n",
    "visualize_prediction(model, image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFFcScDkc_3U"
   },
   "source": [
    "## Understanding Activations\n",
    "\n",
    "**What are activations?** As data flows through a neural network, each layer transforms the input and produces an output called an \"activation\". These activations are the intermediate representations the network builds as it processes information.\n",
    "\n",
    "**Why do they matter?** By examining activations, we can understand:\n",
    "- What features each layer has learned to detect\n",
    "- How the network transforms raw pixels into abstract concepts\n",
    "- Which neurons \"fire\" (have high values) for different inputs\n",
    "\n",
    "Our network has 5 activation points:\n",
    "- **Layer 0**: After first Linear (784 → 32)\n",
    "- **Layer 1**: After first ReLU (32 neurons, negative values zeroed)\n",
    "- **Layer 2**: After second Linear (32 → 16)\n",
    "- **Layer 3**: After second ReLU (16 neurons)\n",
    "- **Layer 4**: Output logits (16 → 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "an-Tqwnudst1",
    "outputId": "1534206f-7e61-4f74-e27c-545bf9453169"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INTERACTIVE ACTIVATION EXPLORER\n",
    "# =============================================================================\n",
    "# Choose how to select your input:\n",
    "#   - By digit (0-9): Shows activations for a sample of that digit\n",
    "#   - By index: Shows activations for a specific sample in the test set\n",
    "\n",
    "# OPTION 1: Select by digit\n",
    "DIGIT_TO_EXPLORE = 5  # Change this! (0-9)\n",
    "\n",
    "image, label = get_sample_by_digit(test_imgs, test_lbls, DIGIT_TO_EXPLORE)\n",
    "plot_activations(model, image, label)\n",
    "\n",
    "# OPTION 2: Select by index (uncomment to use)\n",
    "# INDEX_TO_EXPLORE = 42  # Change this! (0 to ~10000)\n",
    "# image, label = get_sample_by_index(test_imgs, test_lbls, INDEX_TO_EXPLORE)\n",
    "# plot_activations(model, image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARE ACTIVATIONS: Two digits side by side\n",
    "# =============================================================================\n",
    "# See how different digits activate different neurons!\n",
    "\n",
    "COMPARE_DIGIT_1 = 1  # A \"simple\" digit\n",
    "COMPARE_DIGIT_2 = 8  # A \"complex\" digit\n",
    "\n",
    "img1, lbl1 = get_sample_by_digit(test_imgs, test_lbls, COMPARE_DIGIT_1)\n",
    "img2, lbl2 = get_sample_by_digit(test_imgs, test_lbls, COMPARE_DIGIT_2)\n",
    "\n",
    "print(f\"Comparing digit {COMPARE_DIGIT_1} vs digit {COMPARE_DIGIT_2}:\")\n",
    "plot_activations(model, img1, lbl1)\n",
    "plot_activations(model, img2, lbl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Probes\n",
    "\n",
    "**What is a probe?** A probe is a simple classifier (usually linear) trained on the activations of a neural network to detect whether a specific concept is represented in those activations.\n",
    "\n",
    "**Why use probes?** Probes help us understand:\n",
    "- What concepts the network has learned to represent\n",
    "- At which layer these concepts emerge\n",
    "- Whether the network encodes features we didn't explicitly train it on\n",
    "\n",
    "**How it works:**\n",
    "1. Extract activations from a specific layer for many inputs\n",
    "2. Label inputs by the concept we're testing (e.g., \"has a loop\" vs \"no loop\")\n",
    "3. Train a simple linear classifier on these activations\n",
    "4. If the probe achieves high accuracy, the concept is likely encoded in that layer\n",
    "\n",
    "**Example concepts to probe:**\n",
    "- Digits with loops (0, 6, 8, 9) vs without (1, 2, 3, 4, 5, 7)\n",
    "- Even vs odd digits\n",
    "- Any digit pair distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 1: Probe for \"digits with loops\"\n",
    "# =============================================================================\n",
    "# Digits with loops: 0, 6, 8, 9\n",
    "# Digits without loops: 1, 2, 3, 4, 5, 7\n",
    "#\n",
    "# Try changing these groups to test different concepts!\n",
    "\n",
    "POSITIVE_DIGITS = [0, 6, 8, 9]  # Digits WITH loops\n",
    "NEGATIVE_DIGITS = [1, 2, 3, 4, 5, 7]  # Digits WITHOUT loops\n",
    "LAYER_TO_PROBE = 2  # Which layer to probe (0-4)\n",
    "\n",
    "probe, accuracy = train_probe(\n",
    "    model, train_loader, test_loader,\n",
    "    positive_digits=POSITIVE_DIGITS,\n",
    "    negative_digits=NEGATIVE_DIGITS,\n",
    "    layer_num=LAYER_TO_PROBE\n",
    ")\n",
    "\n",
    "print(f\"\\nThe probe can distinguish {POSITIVE_DIGITS} from {NEGATIVE_DIGITS}\")\n",
    "print(f\"at layer {LAYER_TO_PROBE} with {accuracy:.1%} accuracy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what the probe learned (which neurons it relies on)\n",
    "visualize_probe_weights(probe, title=f\"Probe Weights for Layer {LAYER_TO_PROBE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 2: Compare probe accuracy across all layers\n",
    "# =============================================================================\n",
    "# This shows at which layer the concept becomes most detectable\n",
    "\n",
    "results = compare_probes_across_layers(\n",
    "    model, train_loader, test_loader,\n",
    "    positive_digits=POSITIVE_DIGITS,\n",
    "    negative_digits=NEGATIVE_DIGITS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Your Own Experiments!\n",
    "\n",
    "Modify the cells below to test different hypotheses about what the network has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 3: Even vs Odd digits\n",
    "# =============================================================================\n",
    "# Try probing for even vs odd - does the network represent this concept?\n",
    "\n",
    "EVEN_DIGITS = [0, 2, 4, 6, 8]\n",
    "ODD_DIGITS = [1, 3, 5, 7, 9]\n",
    "\n",
    "probe_even_odd, acc_even_odd = train_probe(\n",
    "    model, train_loader, test_loader,\n",
    "    positive_digits=EVEN_DIGITS,\n",
    "    negative_digits=ODD_DIGITS,\n",
    "    layer_num=2  # Try different layers!\n",
    ")\n",
    "\n",
    "print(f\"\\nEven vs Odd probe accuracy: {acc_even_odd:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 4: Single digit pair\n",
    "# =============================================================================\n",
    "# How well can the network distinguish between two specific digits?\n",
    "\n",
    "DIGIT_A = [3]  # Change this!\n",
    "DIGIT_B = [8]  # Change this!\n",
    "\n",
    "probe_pair, acc_pair = train_probe(\n",
    "    model, train_loader, test_loader,\n",
    "    positive_digits=DIGIT_A,\n",
    "    negative_digits=DIGIT_B,\n",
    "    layer_num=2\n",
    ")\n",
    "\n",
    "print(f\"\\n{DIGIT_A} vs {DIGIT_B} probe accuracy: {acc_pair:.1%}\")\n",
    "visualize_probe_weights(probe_pair, title=f\"Probe for {DIGIT_A} vs {DIGIT_B}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 5: Your own hypothesis!\n",
    "# =============================================================================\n",
    "# What other concepts might the network have learned?\n",
    "# Ideas:\n",
    "#   - Digits > 5 vs <= 5\n",
    "#   - Digits with curves (0,2,3,5,6,8,9) vs straight lines (1,4,7)\n",
    "#   - Digits with horizontal lines (2,4,5,7) vs without\n",
    "#\n",
    "# Fill in your own groups below:\n",
    "\n",
    "MY_POSITIVE_DIGITS = []  # Fill this in!\n",
    "MY_NEGATIVE_DIGITS = []  # Fill this in!\n",
    "\n",
    "# Uncomment when ready:\n",
    "# probe_custom, acc_custom = train_probe(\n",
    "#     model, train_loader, test_loader,\n",
    "#     positive_digits=MY_POSITIVE_DIGITS,\n",
    "#     negative_digits=MY_NEGATIVE_DIGITS,\n",
    "#     layer_num=2\n",
    "# )\n",
    "# print(f\"Custom probe accuracy: {acc_custom:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
